{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropia e informação mútua <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "## Informação\n",
    "\n",
    "Teoria da informação é uma área muito interessante que nos forneceu métodos para medir coisas relacionadas à ignorância que temos sobre uma hipótese. Sua compreenssão é fundamental para que possamos criar e compreender diversas métricas usadas em machine learning, como a entropia, e também para desenvolver modelo probabilísticos.\n",
    "\n",
    "Suponha que temos uma variável aleatória $x$ e estamos interessados na quantidade de informação que obteremos ao medir algum valor de $x$. Se o evento (resultado de uma medição é $x$) é altamente improvável, ao medí-lo obteremos mais informação do que se ele for altamente provável. Se você tem certeza de que algo irá acontecer, não terá nada novo (nenhuma informação extra) quando observar esse evento.\n",
    "\n",
    "A informação é portanto, dada por uma quantidade $h(x)$ que é uma função da distribuição de probabilidade $p(x)$. Se observarmos dois eventos, $x$ e $y$, e eles não estiverem relacionados a quantidade total de informação obtida será simplemente a soma da informação dos dois eventos:\n",
    "\n",
    "$$h(x,y) = h(x) + h(y)\\ \\iff \\ p(x,y) = p(x)p(y)$$\n",
    "\n",
    "Obs: $\\iff$ lê-se \"se e somente se\".\n",
    "\n",
    "Como é costume, descrevemos a informação com o logarítmo da distribuição de probabilidade. Se $x$ for uma variável discreta podemos usar a base $2$,\n",
    "\n",
    "$$h(x) = - log_{2}p(x).$$\n",
    "\n",
    "O valor esperado (expectation em inglês, ou esperança se você foi aluno de uma universidade chique) desta quantidade é o que chamamos de `entropia` da variável $x$,\n",
    "\n",
    "$$H[x] = -  \\sum_{x} p(x) log_{2}p(x)$$\n",
    "\n",
    "Obs: *valor esperado* é o termo utilizado na Física.\n",
    "\n",
    "Se a chance de observarmos $x$ é nula, $p(x)=0$, não obteremos informação alguma, $H[x]=0$.\n",
    "\n",
    "Tomemos um exemplo, suponha que observamos a rolagem de um dado não enviesado de oito faces e queremos transmitir a alguém a informação sobre o valor esperado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "H = -8*(1/8)*np.log2(1/8)\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa quantidade é medida em `bits` e nesse caso precisamos de um número de pelo menos 3 `bits` para transmitir essa informação. Caso $x$ seja contínuo é comum usar o logarítmo natural e a unidade de medida passa a ser `nats`.\n",
    "\n",
    "Vejamos um exemplo com três distribuições com a mesma média, mas com diferentes desvios padrões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.array([0.2,0.2,0.2,0.2,0.2])\n",
    "p2 = np.array([0.1,0.15,0.5,0.15,0.1])\n",
    "p3 = np.array([0.0,0.005,0.99,0.005,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(131)\n",
    "plt.bar(np.linspace(0,1,5),p1,alpha=0.7,width=0.24)\n",
    "plt.title(\"p1\")\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.subplot(132)\n",
    "plt.bar(np.linspace(0,1,5),p2,alpha=0.7,width=0.24)\n",
    "plt.title(\"p2\")\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.subplot(133)\n",
    "plt.bar(np.linspace(0,1,5),p3,alpha=0.7,width=0.24)\n",
    "plt.title(\"p3\")\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos uma função para calcular a entropia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6094379124341005, 1.376226604344546, 0.06293300616044681)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def H(p):\n",
    "    return np.sum([-val*np.log(val) if val > 0 else 0 for val in p])\n",
    "\n",
    "H(p1),H(p2),H(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribuição mais estreita tem menor entropia. Isso ocorre pois a incerteza que temos sobre seu valor esperado é menor. Novamente, se alguma distribuição é $ p(x_{i})=1 $ o resto todo será $p(x_{j \\ne i})=0$, ou seja, incerteza nula e informação $H(x_{i})=0$.\n",
    "\n",
    "## Entropia diferencial <a class=\"anchor\" id=\"3-1\"></a>\n",
    "\n",
    "Podemos também usar uma distribuição $p(x)$ contínua e assim a soma discreta se torna uma integral. Neste caso é comum chamar a entropia de `entropia diferencial`:\n",
    "\n",
    "$$H[x] = -\\int p(x) \\ln p(x) dx.$$\n",
    "\n",
    "Algo que acho muito interessante é responder a pergunta *Qual é a distribuição de probabilidade que maximiza a entropia?*\n",
    "\n",
    "A distribuição Gaussiana é aquela que maximiza a entropia diferencial (isso faz sentido, pois é a distribuição favorita da natureza):\n",
    "\n",
    "$$H[x] =  -\\int_{-\\infty}^{+\\infty} \\mathcal{N}(x\\vert \\mu, \\sigma^{2}) \\ln \\mathcal{N}(x\\vert \\mu, \\sigma^{2}) dx = \\frac{1}{2}(1 + \\ln{2 \\pi \\sigma^{2}}).$$\n",
    "\n",
    "Este resultado concorda com o exemplo visto anteriormente, a entropia aumenta conforme a distribuição se torna mais larga, ou seja, $\\sigma^{2}$ aumenta. No fim dessa lição há mais detalhes sobre esse desenvolvimento.\n",
    "\n",
    "## Entropia condicional <a class=\"anchor\" id=\"3-2\"></a>\n",
    "\n",
    "Mudamos agora para o caso em que temos uma distribuição conjunta, digamos $p(x,y)$ e temos uma observação de $x$. Como as duas variáveis estão ligadas de alguma forma é possível calcular a quantidade de informação necessária para especificar a observação correspondente de $y$ com $- \\ln p(y\\vert x)$. Assim, o valor esperado de informação adicional para espeficicar $y$, chamada de `entropia condicional` de $y$ dado $x$ é:\n",
    "\n",
    "$$H[y\\vert x] = -\\int \\int p(y,x) \\ln{ p(y\\vert x)}dydx$$\n",
    "\n",
    "Usando $p(y\\vert x) = \\frac{p(y,x)}{p(x)}$ temos:\n",
    "\n",
    "$$H[y\\vert x] = H[y,x] - H[x].$$\n",
    "\n",
    "## Divergência de Kullback-Leibler <a class=\"anchor\" id=\"3-3\"></a>\n",
    "\n",
    "A classe de métricas conhecidas como divergências são usadas para medir distância entre distribuições de probabilidade, uma das mais famosas é a divergência de Kullback-Liebler (divergência KL ou apenas $D_{KL}$). Vamos ver como construí-la usando os conceitos de entropia.\n",
    "\n",
    "Suponha que temos uma distribuição desconhecida $p(x)$ e nós estamos usando outra distribuição $q(x)$ para modelar a anterior, ou seja, queremos que $q$ igual a $p$. Podemos calcular a quantidade de informação adicional necessária para especificar $x \\sim p(x)$ ao observar $x \\sim q(x)$:\n",
    "\n",
    "$$D_{KL} \\left( p \\vert \\vert q \\right) = -\\int p(x) \\ln q(x) dx + \\int p(x) \\ln p(x) dx = -\\int p(x) \\ln{\\left(\\frac{q(x)}{p(x)} \\right)}dx.$$\n",
    "\n",
    "Essa `entropia relativa` entre as duas distribuição é a `Divergência KL` e é a medida de dissimilaridade entre duas distribuições. Devemos notar que ela é antissimétrica,\n",
    "\n",
    "$$D_{KL} \\left( p \\vert \\vert q \\right) \\ne D_{KL}(q\\vert \\vert p).$$\n",
    "\n",
    "Para aproximar $q(x)$ de $p(x)$ podemos obvservar $x \\sim p(x)$ um número finito de vezes, $N$, usar $q$ como uma função paramétrica, $q(x \\vert \\theta)$ e usar o valor esperado de $D_{KL}(p \\vert \\vert q)$ como função de perda dentro da estratégia de otimização escolhida.\n",
    "\n",
    "O valor esperado de uma função da variável aleatória $x$, $f(x)$, é dada por $\\mathbb{E}[f] = \\int p(x) f(x)dx$ e para um número finito de observações $N$ e integral se torna uma soma finita:\n",
    "\n",
    "$$\\mathbb{E}[f] \\approx \\frac{1}{N} \\sum_{n=1}^{N} f(x_{n}).$$\n",
    "\n",
    "Fazemos isso para obter o valor esperado de $D_{KL}$:\n",
    "\n",
    "$$D_{KL}(p \\vert \\vert q) \\approx \\frac{1}{N} \\sum_{n=1}^{N}(-\\ln q(x_{n} \\vert \\theta) + \\ln p(x_{n})).$$\n",
    "\n",
    "O primeiro termo é o negativo do log da verossimilhança (*`negative log likelihood`*) da distribuição $q(x\\vert \\theta)$ usando um conjunto de parâmetros $\\theta$. É por esse motivo que costuma-se dizer *minimizar a $D_{KL}$ é equivalente a maximizar a função de verossimilhança*.\n",
    "\n",
    "Como um exemplo podemos supor as duas distribuições normais:\n",
    "\n",
    "$$p(x)=\\mathcal{N}(x\\vert \\mu, \\sigma^{2}) \\text{ e } q(x)=\\mathcal{N}(x\\vert m, s^{2})$$\n",
    "\n",
    "para encontrar $D_{KL} \\left( p \\vert \\vert q \\right)$ (o desenvolvimento está no final do post):\n",
    "\n",
    "$$D_{KL} \\left( p \\vert \\vert q \\right) = \\ln{\\frac{s}{\\sigma}} - \\frac{1}{2} + + \\frac{\\sigma^{2}}{2s^{2}}+ \\frac{(\\mu - m)^{2}}{2s^{2}}$$ \n",
    "\n",
    "onde $(\\langle x \\rangle -m)^{2} -\\langle x \\rangle^{2} = -2 \\langle x \\rangle m + m^{2}$. Se as variâncias $\\sigma^{2}$ e $s^{2}$ forem iguais os três primeiros termos se anulam, já se as médias forem iguais o último termo se anula. Isso pode ser usado para saber o quanto uma distribuição se afasta da normal \"pela direita\" ou \"pela esquerda\", devido a propriedade de antissimetria.\n",
    "\n",
    "### f-Divergence <a class=\"anchor\" id=\"3-4\"></a>\n",
    "\n",
    "Como dito antes, a divergência KL é a mais famosa de uma grande família conhecida como $f$-divergências, definidas de maneira mais geral como:\n",
    "\n",
    "$$D_{f}(p \\vert \\vert q) \\equiv \\frac{4}{1 - f^{2}} \\left( 1 - \\int p(x)^{\\left(\\frac{1 + f}{2} \\right)} q(x)^{\\left(\\frac{1 - f}{2} \\right)}dx \\right)$$\n",
    "\n",
    "onde $f$ é um parâmetro contínuo, $-\\infty \\le f \\le + \\infty$. Alguns casos especiais são:\n",
    "\n",
    "$$D_{KL} \\left( p \\vert \\vert q \\right) = \\lim_{f \\rightarrow 1} D_{f}(p \\vert \\vert q),$$\n",
    "\n",
    "$$D_{KL} \\left( q\\vert \\vert p \\right) = \\lim_{f \\rightarrow -1} D_{f}(p \\vert \\vert q)$$\n",
    "\n",
    "e a `distância de Hellinger`:\n",
    "\n",
    "$$D_{H}(p \\vert \\vert q) = \\lim_{f \\rightarrow 0}  D_{f}(p \\vert \\vert q) = \\int \\left( p(x)^{2} - q(x)^{2} \\right)^{2}dx.$$\n",
    "\n",
    "Como as distribuições de probabilidade tem [suporte compacto](https://pt.qwe.wiki/wiki/Support_(mathematics)) $D_{f}(p \\vert \\vert q) \\ge 0$, novamente, a divergência é zero se e somente se $p(x) = q(x)$.\n",
    "\n",
    "## Informação mútua  <a class=\"anchor\" id=\"3-5\"></a>\n",
    "\n",
    "Para um distribuição conjunta $p(y,x)$ a $D_{KL}$ pode ser usada para quantificar o quão próximas duas variáveis estão de serem independentes, ou seja, $p(y,x) = p(y)p(x)$. Isso é chamado de `informação mútua` entre $x$ e $y$:\n",
    "\n",
    "$$I[y,x] = D_{KL}\\left( p(y,x)\\vert \\vert p(y)p(x) \\right) = -\\int \\int p(y,x) \\ln \\frac{p(y)p(x)}{p(y,x)}dydx.$$  \n",
    "\n",
    "$I[y,x] \\ge 0$ e $I[y,x] = 0 \\iff$ $x$ e $y$ são independentes.\n",
    "\n",
    "Por fim a informação mútua pode ser escrita em termos da entropia condicional:\n",
    "\n",
    "$$I[y,x] = H[y] - H[y\\vert x] = H[x] - H[x\\vert y].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medindo entropia\n",
    "\n",
    "### A distribuição que maximiza a entropia <a class=\"anchor\" id=\"8-1\"></a>\n",
    "\n",
    "Como discutido anteriormente, vamos responder: **Qual é a distribuição de probabilidade que maximiza a entropia?**\n",
    "\n",
    "Podemos fazer isso usando a técnica de [`multiplicadores de Lagrange`](https://pt.wikipedia.org/wiki/Multiplicadores_de_Lagrange) restringindo o primeiro e segundo momentos de $p(x)$ juntamente com sua normalização:\n",
    "\n",
    "$$\\int_{-\\infty}^{+\\infty}p(x)dx = 1,$$\n",
    "\n",
    "$$\\int_{-\\infty}^{+\\infty} x p(x)dx = \\mu,$$\n",
    "\n",
    "$$\\int_{-\\infty}^{+\\infty} (x- \\mu)^{2} p(x)dx = \\sigma^{2}.$$\n",
    "\n",
    "Ao fazer a [derivada variacional](https://pt.wikipedia.org/wiki/C%C3%A1lculo_variacional) ir a zero encontraremos a solução $p(x) = \\exp{(-1 + \\lambda_{1} + \\lambda_{2} x + \\lambda_{3} (x - \\mu)^{2})}$. Ao substituir a solução nas equações acima encontraremos:\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\exp{\\left(-\\frac{(x - \\mu)^{2}}{2 \\sigma^{2}}\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $D_{KL} \\left( p(x) \\vert \\vert q(x) \\right)$ entre duas distribuições Normais  <a class=\"anchor\" id=\"8-2\"></a>\n",
    "\n",
    "É possível encontrar uma forma fechada para a divergência $KL$ entre duas distribuições Normais. Vejamos a seguir o desenvolvimento para isso.\n",
    "\n",
    "**Objetivo**: Calcular $D_{KL} \\left( p(x) \\vert \\vert q(x) \\right)$ para $p(x)=\\mathcal{N}(x\\vert \\mu, \\sigma^{2})$ e $q(x)=\\mathcal{N}(x\\vert m, s^{2})$\n",
    "\n",
    "$D_{KL} \\left( p \\vert \\vert q \\right) = -\\int p(x) \\ln{\\frac{q(x)}{p(x)}}dx$  \n",
    "$D_{KL} \\left( p \\vert \\vert q \\right) = -\\int p(x) \\ln{q(x)}dx + \\int p(x) \\ln{p(x)}dx$  \n",
    "$D_{KL} \\left( p \\vert \\vert q \\right) = \\frac{1}{2}\\int p(x) \\ln{2 \\pi s^{2}}dx + \\frac{1}{2s^{2}}\\int p(x)(x-m)^{2}dx - \\frac{1}{2}(1+\\ln{2 \\pi \\sigma^{2}})$  \n",
    "$D_{KL} \\left( p \\vert \\vert q \\right) = \\frac{1}{2}\\ln{2 \\pi s^{2}} - \\frac{1}{2}(1+\\ln{2 \\pi \\sigma^{2}})+ \\frac{1}{2s^{2}}(\\langle x \\rangle^{2} -2m \\langle x \\rangle + m^{2})$  \n",
    "$D_{KL} \\left( p \\vert \\vert q \\right) = \\frac{1}{2} \\ln{\\frac{s^{2}}{\\sigma^{2}}} - \\frac{1}{2} + \\frac{\\sigma^{2}+(\\mu - m)^{2}}{2s^{2}}$  \n",
    "$D_{KL} \\left( p \\vert \\vert q \\right) = \\ln{\\frac{s}{\\sigma}} - \\frac{1}{2} + \\frac{\\sigma^{2}+(\\mu - m)^{2}}{2s^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information <a class=\"anchor\" id=\"4-1\"></a>\n",
    "\n",
    "A PMI é uma medida de associação entre a ocorrência de dois eventos. No nosso caso os eventos são a ocorrência ou não de tokens pertencentes ao corpus.\n",
    "Sejam duas variáveis aleatórias discretas $x \\sim X$ e $y \\sim Y$. A PMI é a distância, no espaço de log, entre as probabilidades conjunta e disjunta destas duas variáveis, em outras palavras, ela mede o quão distante os dois eventos estão de serem independentes. A PMI é máxima se o pair $x,y$ sempre ocorre junto.\n",
    "\n",
    "$PMI(x,y) \\equiv \\log{\\frac{p(x,y)}{p(x)p(y)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio <a class=\"anchor\" id=\"4-2\"></a>\n",
    "\n",
    "Dado que estamos interessados em identificar uma collocation (bi-grama) composta pelos tokens $w^1 w^2$, podemos ter duas hipóteses:\n",
    "\n",
    "- $H_{1}$(Independência): $P(w^{2} \\vert w^{1}) = p = P(w^{2} \\vert \\neg w^{1})$  \n",
    "- $H_{2}$(Dependência): $P(w^{2} \\vert w^{1}) = p_{1} \\neq p_{2} = P(w^{2} \\vert \\neg w^{1})$\n",
    "\n",
    "Sejam $c_{1}$ as ocorrências de $w^{1}$,$c_{2}$ de $w^{2}$, $c_{12}$ de $w^{1}w^{2}$ e $N$ o número total de observações. Temos:\n",
    "\n",
    "$p=\\frac{c_{2}}{N},\\ p_{1}=\\frac{c_{12}}{c_{1}},\\ p_{2}=\\frac{c_{2}-c_{12}}{N-c_{1}}$\n",
    "\n",
    "A ocorrência de um evento deste tipo é dada por uma distribuição binomial (probabilidade de conseguir $k$ sucessos em $n$ experimentos de Bernoulli, sendo que a chance de sucesso em cada é $p$):\n",
    "\n",
    "$B(k;n,p) = \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k}$\n",
    "\n",
    "Assim a likelihood das hipóteses 1 e 2 são:\n",
    "\n",
    "$L(H_{1})=B(c_{12};c_{1},p)B(c_{2}-c_{12};N-c_{1},p)$  \n",
    "\n",
    "$L(H_{2})=B(c_{12};c_{1},p_{1})B(c_{2}-c_{12};N-c_{1},p_{2})$\n",
    "\n",
    "No fim das contas, a razão entre as likelihoods é outra maneira de medir o quanto a ocorrência entre os dois tokens estão de ser independentes:\n",
    "\n",
    "$\\lambda=\\frac{L(H_{1})}{L(H_{2})}$.\n",
    "\n",
    "Um valor $\\lambda=1$ significa que as duas hipóteses são igualmente prováveis, já $\\lambda < 1$ indica que a hipótese 2 (dependência) é a mais provável."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
