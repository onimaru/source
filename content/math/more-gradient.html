
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>More on Gradients &#8212; Disorder Transform</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Disorder Transform</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Disorder Transform
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Math
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   1. Theoretical minimum
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability-distributions.html">
   2. Distribuições de probabilidade
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="information-theory.html">
   3. Entropia e informação mútua
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simple-example-of-maximum-likelihood.html">
   4. Maximum Likelihood Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eigen-decomposition.html">
   5. Decomposição de matrizes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient-descent.html">
   6. Gradiente descendente
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lagrange-multipliers.html">
   7. Multiplicadores de Lagrange
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="method-of-kernels.html">
   8. Method of kernels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="integer-programming-resource.html">
   9. Integer Programming
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/bias-variance.html">
   1. Bias and Variance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/understanding-regularization.html">
   2. Understanding regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/laplace-approximation.html">
   3. Laplace Approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/computing-hessian-and-jacobian.html">
   4. Computing Hessian and Jacobian
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/counter-factual-model.html">
   5. Counter Factual Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/intro-manifold.html">
   7. Introdução ao Manifold Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/multidimensional-scaling.html">
   8. Manifold Learning - Multidimensional Scaling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/active-learning-strategy.html">
   9. Active Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/k-means.html">
   10. Expectation-Maximization Algorithm - k-Means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/dataloader-example.html">
   11. Dataloader example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/nn-universal-approximator.html">
   12. Neural Network as an Universal Approximator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning/variational-autoencoder.html">
   13. Variational AutoEncoder
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Ranking Problems
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ranking/how-to-ltr.html">
   1. How to Learn to Rank
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ranking/ranknet.html">
   2. Ranknet
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ranking/ranknet-teacher-student.html">
   3. Ranknet - Teacher-Student strategy
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../reinforcement-learning/intro.html">
   1. Beyond conventional machine learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reinforcement-learning/bandits-ab-test-regret.html">
   2. AB test and the regret
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reinforcement-learning/k-armed-bandits.html">
   3. The Multi-Armed Bandits: Exploitation vs Exploration
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Differential Geometry
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../differential-geometry/intro.html">
   1. Problems and solutions in differential geometry
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../differential-geometry/maps.html">
   2. Mapas, Curvas, superfícies e Variedades
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../differential-geometry/vector-spaces.html">
   3. Espaços Vetoriais
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../differential-geometry/algebras.html">
   4. Álgebras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../differential-geometry/metric-tensor-fields.html">
   5. Campos Tensoriais Métricos
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Extra content
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../extra/definitions.html">
   Definições
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../extra/symbol-index.html">
   Lista de símbolos
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Datasets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../datasets/ranking-teams.html">
   Ranking dataset - Teams
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/math/more-gradient.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/onimaru/source"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/onimaru/source/issues/new?title=Issue%20on%20page%20%2Fcontent/math/more-gradient.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="more-on-gradients">
<span id="more-gradient"></span><h1>More on Gradients<a class="headerlink" href="#more-on-gradients" title="Permalink to this headline">¶</a></h1>
<p>Since there are many algorithms based on gradient descent, the gradient computation of many quantities can appear. So, here is some of common ones.</p>
<p>Let <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n}\)</span> be a model parameter vector. Let <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n}\)</span> a feature input vector.</p>
<ol class="simple">
<li><p>Let the map <span class="math notranslate nohighlight">\(L: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span> be the loss function which maps parameter vectors to the real line, define as <span class="math notranslate nohighlight">\(L(w)=x^{\top}w=x_{i}w^{i}\)</span>. We write the gradient of <span class="math notranslate nohighlight">\(L\)</span> as:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \nabla L(w) &amp;= \frac{\partial}{\partial w_{j}} \left(x_{i}w^{i}\right) \\
              &amp;= \partial^{j} x_{i}w^{i} \\
              &amp;= \partial^{j}(x_{i})w^{i} + x_{i}\partial^{j}w^{i}\\
              &amp;= 0 + x_{i} \delta^{jl}\partial_{l}w^{i} \\
              &amp;= x_{i} \delta^{jl} \delta_{l}^{i} \\
              &amp;= x_{i} \delta^{ji} \\
              &amp;= x_{i} \delta^{ij} \\
              &amp;= x^{j}
\end{align}\end{split}\]</div>
<p>where the quantity <span class="math notranslate nohighlight">\(\delta_{l}^{i}=\frac{\partial w^{i}}{\partial w^{l}}\)</span> is the Kronecker delta and <span class="math notranslate nohighlight">\(\delta^{jl}=\delta^{lj}\)</span> is the Euclidian space metric tensor.</p>
<ol class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times n}\)</span> be a square matrix of constant elements with respect to the model parameters. Let the loss function be defined as <span class="math notranslate nohighlight">\(L(w)= w^{\top}Xw\)</span>.</p></li>
</ol>
<p>Let us rewrite the loss in index notation as <span class="math notranslate nohighlight">\(L(w) = w_{i}X^{i}_{j}w^{j}\)</span>, then its gradient is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \nabla L(w) &amp;= \frac{\partial}{\partial w_{k}} \left(w_{i}X^{i}_{j}w^{j}\right) \\
              &amp;= \partial^{k} (w_{i}) X^{i}_{j} w^{j} + w_{i} X^{i}_{j} \partial^{k} w^{j} \\
              &amp;= \delta^{k}_{i} X^{i}_{j} w^{j} + w_{i} X^{i}_{j} \delta^{kj} \\
              &amp;= X^{k}_{j} w^{j} + w_{i} X^{ik} \\
              &amp;= X^{k}_{j} w^{j} + \delta_{ij} X^{ik} w^{j} \\
              &amp;= X^{k}_{j} w^{j} + X_{j}^{k} w^{j} \\
              &amp;= \left(X^{k}_{j} + X_{j}^{k} \right) w^{j}
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(X_{j}^{k}\)</span> is the transpose of <span class="math notranslate nohighlight">\(X^{k}_{j}\)</span> and we also used <span class="math notranslate nohighlight">\(w_{i} = \delta_{ij}w^{j}\)</span>.</p>
<ol class="simple">
<li><p>Let the loss function be defined as <span class="math notranslate nohighlight">\(L(w) = w^{\top}w\)</span>.</p></li>
</ol>
<p>The function <span class="math notranslate nohighlight">\(L\)</span> is an inner product, but since we treat the <span class="math notranslate nohighlight">\(w\)</span> vectors as tensors, this is the same as an outer product: <span class="math notranslate nohighlight">\(w^{\top}w= w^{\top} \otimes w = w_{i} \delta^{i}_{j}w^{j}\)</span> (I am sorry for the digression). Thus, the gradient is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \nabla L(w) &amp;= \frac{\partial}{\partial w_{k}} \left(w_{i}w^{i}\right) \\
              &amp;= \left(\partial^{k} w_{i} \right) w^{i} + w_{i} \left(\partial^{k} w^{i} \right) \\
              &amp;= \delta^{k}_{i} w^{i} + w_{i}\delta^{ki} \\
              &amp;= w^{k} + w^{k} \\
              &amp;= 2 w^{k}
\end{align}\end{split}\]</div>
<ol class="simple">
<li><p>Let the loss function be defined as the norm <span class="math notranslate nohighlight">\(L(w) = \vert \vert w \vert \vert_{2}\)</span>.</p></li>
</ol>
<p>Here we need to use the chain rule of derivatives, since <span class="math notranslate nohighlight">\(\vert \vert w \vert \vert_{2} = \sqrt{w^{\top}w}\)</span>. So we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \nabla L(w) &amp;= \frac{\partial}{\partial w_{k}} \left(w_{i}w^{i}\right)^{1/2} \\
              &amp;= \frac{1}{2} \left(w_{i}w^{i}\right)^{-1/2} \partial^{k} \left(w_{i}w^{i}\right) \\
              &amp;= \frac{1}{2 \vert \vert w \vert \vert_{2}} 2w^{k}\\
              &amp;=  \frac{w^{k}}{\vert \vert w \vert \vert_{2}}
\end{align}\end{split}\]</div>
<p>Each component of the gradient of the norm is the relation between the component and the norm, which makes sense.</p>
<ol class="simple">
<li><p>Let the loss function be defined as the norm <span class="math notranslate nohighlight">\(L(w) = f\left(\vert \vert w \vert \vert_{2} \right)\)</span>, where <span class="math notranslate nohighlight">\(f\)</span> is a function <span class="math notranslate nohighlight">\(f: \mathbb{R} \rightarrow \mathbb{R}\)</span>.</p></li>
</ol>
<p>This is very similar to the last case, actually it is more general.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \nabla L(w) &amp;= \frac{\partial}{\partial w_{k}} f\left(\vert \vert w \vert \vert_{2} \right) \\
              &amp;= \frac{df}{d \vert \vert w \vert \vert_{2}} \partial^{k}\left(\vert \vert w \vert \vert_{2} \right) \\
              &amp;= \frac{df}{d \vert \vert w \vert \vert_{2}} \frac{w^{k}}{\vert \vert w \vert \vert_{2}}
\end{align}\end{split}\]</div>
<p>That is it for now. Sometimes when implementing some gradient based algorithm it is useful to know before hand if the gradient can be written explicitly. This can save us some time and not depend too much on auto-grad frameworks.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/math"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By onimaru<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>