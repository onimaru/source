(reinforcement-learning)=
# Beyond conventional machine learning

üèóÔ∏è


## Processos de Decisao de Markov Finitos (finite MDPs)

MDPs s√£o a formaliza√ß√£o cl√°ssica da tomada de decis√£o sequential, onde a√ß√µes influenciam n√£o s√≥ as recompensas imediatas, mas tamb√©m  situa√ß√µes subsequentes, ou estados, e recompensas futuras.

## A interface Agente-Ambiente

Aquele que aprende e √© o tomador de decis√µes √© chamado de agente (agent). a coisa com a qual ele interage, composta de tudo aquilo fora do agente, √© chamado de ambiente (environment).

A figura abaixo mostra a forma da intera√ß√£o entre os dois.

**Inserir imagem**

Os dois interagem a cada sequ√™ncia de passos de um tempo discreto, $t$. A cada passo $t$ o agente recebe uma representa√ß√£o do estado do ambiente, $S_{t}\in \mathcal{S}$, e baseado nisso seleciona uma a√ß√£o $A_{t}\in \mathcal{A}(s)$. No passo de tempo seguinte o agente, como consequ√™ncia de sua a√ß√£o, recebe um valor num√©rico como recompensa, $R_{t+1}\in \mathcal{R} \subset \mathbb{R}$, e se encontra em um novo estado, $S_{t+1}$. O MDP e o agente juntos d√£o assim origem a uma sequ√™ncia ou trajet√≥ria como esta:

$$S_{0},A_{0},R_{1},S_{1},A_{1},R_{2},S_{2},A_{2},R_{3},...$$

Em um MDP finito os conjuntos $\mathcal{S},\mathcal{A}$ e $\mathcal{R}$ t√™m n√∫mero de elementos finitos. Nesse caso, as vari√°veis aleat√≥rias $R_{t}$ e $S_{t}$ t√™m definidas distribui√ß√µes de probabilidade discretas dependente somente do estado e da a√ß√£o anteriores. Assim:

$$p(s^{'},r \vert s,a) = P \lbrace S_{t}=s^{'},R_{t}=r \vert S_{t-1}=s,A_{t-1}=a \rbrace,\ \forall s^{'},s \in \mathcal{S}, r \in \mathcal{R}\ \text{e } a \in \mathcal{A}.$$

Perceba que como $p$ √© uma probabilidade:

$$\sum_{s^{'}} \sum_{r} p(s^{'},r \vert s,a) = 1,\ \forall s \in \mathcal{S} \text{ e } a \in \mathcal{A}.$$

Em um MDP a probabilidades dadas por $p$ caracterizam completamente a din√¢mica do ambiente e atrav√©s dela podemos obter outras quantidades como:
- Probabilidades de transi√ß√£o de estado, $p:\mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$

$$p(s^{'} \vert s,a) = P \lbrace S_{t}=s^{'}\vert S_{t-1}=s,A_{t-1}=a \rbrace = \sum_{r}p(s^{'},r \vert s,a)$$

- Recompensa esperada para o par estado-a√ß√£o, $r:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$

$$r(s,a) = \mathbb{E}\left[ R_{t} \vert S_{t-1}=s,A_{t-1}=a \right] = \sum_{r} r \sum_{s^{'}} p(s^{'},r \vert s,a)$$

- Recompensa esperada para a tripla estado-a√ß√£o-estado seguinte, $r:\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$

$$r(s,a,s^{'}) = \mathbb{E}\left[ R_{t} \vert S_{t-1}=s,A_{t-1}=a, S_{t}=s^{'} \right] = \sum_{r} r \frac{p(s^{'},r \vert s,a)}{p(s^{'} \vert s,a)}.$$

Em resumo o MDP √© um framework abstrato e flex√≠vel que pode ser aplicado a problemas variados de diversas maneiras.

## Retornos e epis√≥dios


Em geral se quer que o agente maximize uma quantidade chamada retorno esperado (expectede return), $G_{t}$, que √© definida como alguma fun√ß√£o da sequ√™ncia de recompensas que pode ser apenas uma soma
$$G_{t} = R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T},$$
onde $T$ √© o passo final.

No caso de uma tarefa n√£o epis√≥dica, tarefa cont√≠nua, precisamos de um conceito adicional, o retorno descontado:
$$G_{t} = R_{t+1}+\gamma R_{t+2}+ \gamma^{2} R_{t+3}+... = \sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1},$$
onde $\gamma$ √© um par√¢metro, $0 \le \gamma \le 1$, chamado taxa de desconto.

Podemos obter uma rela√ß√£o √∫til com
$$G_{t} = R_{t+1}+\gamma R_{t+2}+ \gamma^{2} R_{t+3}+...$$
$$G_{t} = R_{t+1}+\gamma \left( R_{t+2}+ \gamma R_{t+3}+... \right)$$
$$G_{t} = R_{t+1}+\gamma G_{t+1}$$
Assim o retorno descontado pode tamb√©m ser usado em tarefas epis√≥dicas. Indo mais al√©m podem definir:
$$G_{t} = \sum_{k=t+1}^{T}\gamma^{k-t-1} R_{k}$$
Assim $T$ pode ser infinito ou $\gamma =1$, mas √£o ambos.

## Pol√≠ticas e fun√ß√µes de valor


Em v√°rios algoritmos precisamos de fun√ß√µes para estimar o valor de um estado ou do par estado-a√ß√£o. Seu papel √© fornecer informa√ß√£o sobre o quanto a entrada √© valiosa para o agente. Essa fun√ß√µes de valor s√£o definidas de modo a respeitar algum padr√£o de comportamento, chamado de pol√≠tica (policy).

Formalmente uma pol√≠tica √© um mapeamento de estados para probabilidades de selecionar cada poss√≠vel a√ß√£o. Se um agente segue uma pol√≠tica $\pi$ no tempo $t$, ent√£o $\pi(a \vert s)$ √© a probabilidade de $A_{t}=a$ se $S_{t}=s$.

Denotamos a fun√ß√£o de valor de um estado sob a pol√≠tica $\pi$ como $v_{\pi}(s)$. Essa √© o retorno esperado se inicia-se em $s$ seguindo a pol√≠tica $\pi$ ap√≥s isso:
$$v_{\pi}(s) = \mathbb{E}_{\pi} \left[ G_{t} \vert S_{t}=s \right] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \vert S_{t} \right],\ \forall s \in \mathcal{S}.$$
Desta maneira o valor do estado final √© sempre zero. Utilizando umas das rela√ß√µes acima podemos escrever:
$$v_{\pi}(s) = \sum_{a} \pi(a \vert s) \sum_{s^{'},r} p(s^{'},r \vert s,a)\left[r + \gamma v_{\pi}(s^{'}) \right]$$
que √© chamada de equa√ß√£o de Bellman para $v_{\pi}$.

De modo similar, o valor de tomar a a√ß√£o $a$ em um estado $s$, seguindo a pol√≠tica $\pi$, $q_{\pi}(s,a)$, √©:

$$q_{\pi}(s,a) = \mathbb{E}_{\pi} \left[ G_{t} \vert S_{t}=s,A_{t}=a \right] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \vert S_{t}, A_{t}=a \right],\ \forall s \in \mathcal{S}.$$

Usualmente $v_{\pi}(s)$ √© chamada de fun√ß√£o de valor de estado e $q_{\pi}(s,a)$ de fun√ß√£o de valor de estado-a√ß√£o. Essas quantidades podem ser estimadas atrav√©s da experi√™ncia do agente.


---

Uma policy pode ser determin√≠stica, denotada por $\mu$:

$$a_{t}=\mu_{\theta}(s_{t})$$

ou estoc√°stica, usualmente denotada por $\pi$:

$$a_{t} \sim \pi_{\theta}(\cdot \vert s_{t})$$

O s√≠mbolo $\theta$ representa os par√¢metros das policies, usualmente s√£o pesos de uma rede neural.

## Policies determin√≠sticas

Exemplo: uma rede neural com duas camadas de tamanho 64 e ativa√ß√£o `tanh`. 
```python
pi_net = nn.Sequential(
              nn.Linear(obs_dim, 64),
              nn.Tanh(),
              nn.Linear(64, 64),
              nn.Tanh(),
              nn.Linear(64, act_dim)
            )
```

Se `obs` for uma matriz do numpy contendo um batch de observa√ß√µes, podemos obter as a√ß√µes da seguinte maneira:

```python
obs_tensor = torch.as_tensor(obs, dtype=torch.float32)
actions = pi_net(obs_tensor)
```
## Policies estoc√°sticas

Os tipos mais comuns s√£o policies categoricas (para espa√ßos de a√ß√£o discretos) e gaussianas diagonais (para espa√ßos de a√ß√£o cont√≠nuos).

Dois c√°lculos s√£o importantes para usar e treinar policies estoc√°sticas:
- amostrar (sampling) a√ß√µes da policy
- calcular log likelihoods de a√ß√µes particulares, $\log{\pi_{\theta}(a \vert s)}$.

### Policies categ√≥ricas

*Sampling*: dadas as probabilidades para cada a√ß√£o, frameworks como PyTorch e Tensorflow possuem ferramentas para fazer a amostragem.

*Log-Likelihood*: Denotando a √∫ltima camada de probabilidades como $P_{\theta}(s)$. √â um vetor com tantas entradas quantas forem as a√ß√µes, ent√£o as a√ß√µes podem ser tratadas como √≠ndices desse vetor. A log likelihood para uma a√ß√£o `a` pode ser obtida indexando o vetor:

$$\log{\pi_{\theta}(a \vert s_{t})} = \log{P_{\theta}(s)}_{a}$$

### Policies gaussianas diagonais

Uma distribui√ß√£o gaussiana multivariada √© descrita por um vetor m√©dio, $\mu$, e uma matriz de covari√¢ncia, $\Sigma$. No caso da distribui√ß√£o diagonal a matriz de covari√¢ncia tem apenas entradas na diagonal principal.

Uma policy desse tipo tem uma rede neural que mapeia observa√ß√µes para a√ß√µes m√©dias, $\mu_{\theta}(s)$. A vari√¢ncia usualmente √© transformada no log dos desvios padr√µes que, por sua vez, pode ser um par√¢metro espec√≠fico ou ser tb mapeada por uma rede neural.

*Sampling*: Dados o vetor m√©dio, o desvio padr√£o e um vetor $z$ de ru√≠do de uma esfera gaussiana, $z \sum \mathbb{N}(0,I)$, a amostragem de uma a√ß√£o pode ser calculada com:

$$a = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot z$$

*Log-Likelihood*: A log-likelihood de uma a√ß√£o $a$ $k$-dimensional pode ser calculada por:

$$\log{\pi_{\theta}(a \vert s_{t})} = \frac{-1}{2}\left(\sum_{i=1}^{k} \left(\frac{(a_{i}-\mu_{i})^{2}}{\sigma_{i}^{2}} +2 \log{\sigma_{i}} \right) +k \log{2 \pi}\right)$$

## Trajet√≥rias

Uma trajet√≥ria, $\tau$, √© uma sequ√™ncia de estados e a√ß√µes: 


$$\tau = (s_{0},a_{0},s_{1},a_{1},...)$$

As transi√ß√µes de estado podem ser determin√≠sticas, 

$s_{t+1} = f(s_{t},a_{t})$

ou estoc√°sticas

$s_{t+1} \sim P(\cdot \vert s_{t},a_{t})$

